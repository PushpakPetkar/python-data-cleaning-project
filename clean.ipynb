{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"bGcvWhaycKQA"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","source":["#!/usr/bin/env python\n","# coding: utf-8\n","\n","# # Clustering\n","\n","# ## Problem Statement\n","\n","# Students have to evaluate a lot of factors before taking a decision to join a university for their higher education requirements.\n","# The objective of this project is to simplify the process of applying for appropriate universities which are of similar nature.\n","\n","# `CRISP-ML(Q)` process model describes six phases:\n","#\n","# 1. Business and Data Understanding\n","# 2. Data Preparation\n","# 3. Model Building\n","# 4. Model Evaluation\n","# 5. Model Deployment\n","# 6. Monitoring and Maintenance\n","\n","# **Objective(s):** Maximize the convenience of admission process\n","#\n","# **Constraints:** Minimize the brain drain\n","#\n","\n","# **Success Criteria**\n","#\n","# - **Business Success Criteria**: Reduce the application process time from anywhere between 20% to 40%\n","#\n","# - **ML Success Criteria**: Achieve Silhoutte coefficient of atleast 0.6\n","#\n","# - **Economic Success Criteria**: US Higher education department will see an increase in revenues by atleast 30%\n","#\n","# **Proposed Plan:**\n","# Grouping the available universities will allow to understand the characteristics of each group.\n","\n","# In[ ]:\n","\n","\n","\n","\n","\n","# ## Data Collection\n","\n","# Data:\n","#    The university details are obtained from the US Higher Education Body and is publicly available for students to access.\n","#\n","# Data Dictionary:\n","# - Dataset contains 25 university details\n","# - 7 features are recorded for each university\n","#\n","# Description:\n","# - Univ - University Name\n","# - State - Location (state) of the university\n","# - SAT - Average SAT score for eligibility\n","# - Top10 - % of students who ranked in top 10 in their previous academics\n","# - Accept - % of students admitted to the universities\n","# - SFRatio - Student to Faculty ratio\n","# - Expenses - Overall cost in USD\n","# - GradRate - % of students who graduate\n","\n","# In[ ]:\n","\n","\n","get_ipython().system('pip install feature_engine')\n","\n","\n","# **Importing required packages**\n","\n","# In[1]:\n","\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","from sklearn import preprocessing\n","from feature_engine.outliers import Winsorizer\n","\n","\n","# **Import the data**\n","\n","# In[2]:\n","\n","\n","df = pd.read_excel(r'C:\\Users\\Bharani Kumar\\Downloads\\University_Clustering (3).xlsx')\n","df\n","\n","\n","# In[3]:\n","\n","\n","df.info()\n","\n","\n","# ## EXPLORATORY DATA ANALYSIS (EDA) / DESCRIPTIVE STATISTICS\n","\n","# ***1st Moment Business Decision (Measures of Central Tendency)***\n","#\n","# 1) Mean\n","#\n","# 2) Median\n","#\n","# 3) Mode\n","\n","# In[4]:\n","\n","\n","print(df.mean())\n","print('\\n')\n","print(df.median())\n","print('\\n')\n","print(df.mode())\n","\n","\n","# ***2nd Moment Business Decision (Measures of Dispersion)***\n","#\n","# 1) Variance\n","#\n","# 2) Standard deviation\n","#\n","# 3) Range (maximum - minimum)\n","\n","# In[5]:\n","\n","\n","print(df.var())\n","print('\\n')\n","print(df.std())\n","\n","\n","# ***3rd Business Moment Decision (Skewness)***\n","#\n","# Measure of asymmetry in the data distribution\n","\n","# In[6]:\n","\n","\n","df.skew()\n","\n","\n","# ***4th Business Moment Decision (Kurtosis)***\n","#\n","# Measure of peakedness - represents the overall spread in the data\n","\n","# In[7]:\n","\n","\n","df.kurt()\n","\n","\n","# ***Descriptive Statistics and Data Distribution Function***\n","\n","# In[8]:\n","\n","\n","df.describe()\n","\n","\n","# In[ ]:\n","\n","\n","\n","\n","\n","# ## Data Preprocessing and Cleaning\n","\n","# **Typecasting** :\n","#\n","# As Python automatically interprets the data types, there may be a requirement for the data type to be converted. The process of converting one data type to another data type is called Typecasting.\n","#\n","# Example:\n","# 1) int to float\n","#\n","# 2) float to int\n","\n","# In[9]:\n","\n","\n","df.dtypes\n","\n","\n","# The dataset contains UnivID which is being interpreted as Integer by Python. UnivID is an identity and a unique number given to each university. Hence it should be treated as a categorical data. We can convert the integer data to string type.\n","\n","# In[10]:\n","\n","\n","# Convert 'int64' to 'str' (string) type.\n","\n","df.UnivID = df.UnivID.astype('str')\n","df.dtypes\n","\n","\n","# **Cleaning Unwanted columns**\n","#\n","# UnivID is an identity to each university. Analytically it does not have any value (Nominal data). We can safely ignore the ID column by dropping the column.\n","\n","# In[11]:\n","\n","\n","df.drop(['UnivID'], axis = 1, inplace = True)\n","\n","\n","# In[12]:\n","\n","\n","df.info()\n","\n","\n","# In[ ]:\n","\n","\n","\n","\n","\n","# **Handling duplicates:**\n","#\n","# If the dataset has multiple entries of the same record then we can remove the duplicate entries. In our dataset we don't have duplicates. In case of duplicates we will use function drop_duplicates()\n","\n","# In[13]:\n","\n","\n","duplicate = df.duplicated()  # Returns Boolean Series denoting duplicate rows.\n","print(duplicate)\n","sum(duplicate)\n","\n","\n","# In[14]:\n","\n","\n","# Removing Duplicates\n","df = df.drop_duplicates() # Returns DataFrame with duplicate rows removed.\n","\n","\n","# **Missing Value Analysis**\n","#\n","# ***IMPUTATION:***\n","#\n","# The process of dealing with missing values is called Imputation.\n","#\n","# Most popular substitution based Imputation techniques are:\n","#\n","# 1) Mean imputation for numeric data\n","#\n","# 2) Mode imputation for non-numeric data\n","\n","# In[15]:\n","\n","\n","df.isnull().sum() # Check for missing values\n","\n","\n","# In[16]:\n","\n","\n","type(df[\"SAT\"])\n","\n","\n","# In[17]:\n","\n","\n","type(df[[\"SAT\"]])\n","\n","\n","# In[18]:\n","\n","\n","from sklearn.impute import SimpleImputer\n","\n","# Mean Imputer\n","mean_imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n","df[\"SAT\"] = pd.DataFrame(mean_imputer.fit_transform(df[[\"SAT\"]]))\n","\n","df[\"SAT\"].isna().sum()\n","\n","\n","# In[19]:\n","\n","\n","# Median Imputer\n","median_imputer = SimpleImputer(missing_values = np.nan, strategy = 'median')\n","df[\"SFRatio\"] = pd.DataFrame(median_imputer.fit_transform(df[[\"SFRatio\"]]))\n","\n","df[\"SFRatio\"].isna().sum()  # all records replaced by median\n","\n","\n","# In[20]:\n","\n","\n","# Random Imputer\n","from feature_engine.imputation import RandomSampleImputer\n","\n","random_imputer = RandomSampleImputer(['GradRate'])\n","df[\"GradRate\"] = pd.DataFrame(random_imputer.fit_transform(df[[\"GradRate\"]]))\n","\n","df[\"GradRate\"].isna().sum()  # all records replaced by a random value\n","\n","\n","# In[21]:\n","\n","\n","df.isna().sum()\n","\n","\n","# ## Single code for Mean Imputation on the entire dataset.\n","# #### Alternative option\n","\n","# In[22]:\n","\n","\n","df.fillna(df.mean(), inplace = True)\n","\n","\n","# **Outliers Analysis**:\n","#\n","# Extreme data values in a variable can be outliers. In case of outliers we can use one of the strategies of 3 R (Rectify, Retain, or Remove)\n","\n","# **Dividing Categorical and Numerical variables from dataset**\n","\n","# In[23]:\n","\n","\n","df_cate = df.iloc[:, :2]\n","df_cate.head()\n","\n","\n","# In[24]:\n","\n","\n","df_num = df.iloc[:, 2:]\n","df_num.head()\n","\n","\n","# **Box Plot**\n","#\n","# Visualize numeric data using boxplot for outliers\n","\n","# In[25]:\n","\n","\n","# Multiple boxplots in a single visualization.\n","# Columns with larger scales affect other columns.\n","# Below code ensures each column gets its own y-axis.\n","\n","# pandas plot() function with parameters kind = 'box' and subplots = True\n","\n","df_num.plot(kind = 'box', subplots = True, sharey = False, figsize = (10, 6))\n","\n","'''sharey True or 'all': x- or y-axis will be shared among all subplots.\n","False or 'none': each subplot x- or y-axis will be independent.'''\n","\n","\n","# increase spacing between subplots\n","plt.subplots_adjust(wspace = 0.75) # ws is the width of the padding between subplots, as a fraction of the average Axes width.\n","plt.show()\n","\n","\n","# Boxplots show outliers in: SAT, Top10, Accept, SFRatio.\n","#\n","# ***Outlier Analysis***:\n","# We shall use Winsorization Technique to treat outliers\n","#\n","# Winsorization function rounds off the exceptional data points based on capping method used in parameters/hyperparameters section.\n","\n","# In[26]:\n","\n","\n","# D-Tale\n","########\n","\n","# pip install dtale\n","import dtale\n","\n","d = dtale.show(df)\n","d.open_browser()\n","\n","\n","# #### Install the required package if it is not already available\n","# ***Package to get winsorization function***\n","#\n","# !pip install feature_engine\n","\n","# In[27]:\n","\n","\n","# Winsorization for \"SAT, Top10, Accept, SFRatio\" column\n","winsor = Winsorizer(capping_method = 'iqr', # choose IQR rule boundary\n","                          tail = 'both', # cap left, right or both tails\n","                          fold = 1.5,\n","                          variables = ['SAT'])\n","\n","df_num['SAT'] = winsor.fit_transform(df_num[['SAT']]) # this is replacing the outliers to cap values\n","\n","\n","# In[28]:\n","\n","\n","# Winsorization for \"Top10\" column\n","winsor = Winsorizer(capping_method = 'iqr', # choose IQR rule boundary\n","                          tail = 'both', # cap left, right or both tails\n","                          fold = 1.5,\n","                          variables = ['Top10'])\n","\n","df_num['Top10'] = winsor.fit_transform(df_num[['Top10']]) # this is replacing the outliers to cap values\n","\n","\n","# In[29]:\n","\n","\n","# Winsorization for \"Accept\" column\n","winsor = Winsorizer(capping_method = 'iqr', # choose IQR rule boundary\n","                          tail = 'both', # cap left, right or both tails\n","                          fold = 1.5,\n","                          variables = ['Accept'])\n","\n","df_num['Accept'] = winsor.fit_transform(df_num[['Accept']]) # this is replacing the outliers to cap values\n","\n","\n","# In[30]:\n","\n","\n","# Winsorization for \"SFRatio\" column\n","winsor = Winsorizer(capping_method = 'iqr', # choose IQR rule boundaries\n","                          tail = 'both', # cap left, right or both tails\n","                          fold = 1.5,\n","                          variables = ['SFRatio'])\n","\n","df_num['SFRatio'] = winsor.fit_transform(df_num[['SFRatio']]) # this is replacing the outliers to cap values\n","\n","\n","# #### Verify for outliers post the treatment\n","\n","# In[31]:\n","\n","\n","df_num.plot(kind = 'box', subplots = True, sharey = False, figsize = (10, 6)) # sharey = True shares the Y axis across all plots\n","\n","\n","# increase spacing between subplots\n","plt.subplots_adjust(wspace = 0.75)\n","plt.show()\n","\n","\n","# **Zero and near zero variance**\n","#\n","#    We dont have any values with near zero (or) zero variance. We can proceed with further data preprocessing steps.\n","\n","# In[32]:\n","\n","\n","df_num.var()\n","\n","\n","# **Discretization / Binning / Grouping:**\n","#\n","# The process of converting continuous data into discrete number of bins.\n","#\n","# The current business requirement does not justify the discretization/binning requirement for any column.\n","\n","# ### Dummy Variable Creation\n","\n","# In[33]:\n","\n","\n","df.dtypes  #Verify the data types of the fields for the entire data\n","\n","\n","# In[34]:\n","\n","\n","# Alternatively we can use:\n","\n","df.info()\n","\n","\n","# In[35]:\n","\n","\n","df_cate.info()\n","\n","\n","# In[36]:\n","\n","\n","print(df_cate.State.unique())\n","print('\\n')\n","\n","print(df_cate.State.value_counts())\n","\n","\n","# - Univ name is an identity and does not require to be converted into numeric values.\n","#\n","# - STATE column is an important field in decision making hence it is to be converted into Numeric values.\n","\n","# In[37]:\n","\n","\n","# Applying One-Hot Encoding with get_dummies\n","\n","df_cate = pd.get_dummies(df_cate, columns = ['State'], drop_first = True)\n","\n","\n","# In[38]:\n","\n","\n","df_cate.info()\n","\n","\n","# In[39]:\n","\n","\n","# The dimension of the dataset after One-Hot Encoding\n","\n","df_cate.shape\n","\n","\n","# In[40]:\n","\n","\n","df_cate.head()\n","\n","\n","# In[ ]:\n","\n","\n","\n","\n","\n","# # **Graphical Representation**\n","\n","# In[2]:\n","\n","\n","pip install PyQt5 pyqtwebengine --user # run this if you get any warning msg and if the plots are not visible\n","\n","\n","# In[85]:\n","\n","\n","get_ipython().run_line_magic('matplotlib', 'inline')\n","\n","\n","# In[86]:\n","\n","\n","df_num.columns\n","\n","\n","# In[52]:\n","\n","\n","# Histogram describes the spread of data\n","\n","for i in df_num.columns:\n","  plt.hist(x = df_num[i])\n","  plt.title(\"Histogram for \" + str(i))\n","  plt.show()\n","\n","\n","# In[53]:\n","\n","\n","# Normal Quantile-Quantile Plot - Helps in understanding the distribution of the data - Normal distribution or not\n","import scipy.stats as stats\n","import pylab\n","\n","# Checking whether data is normally distributed\n","stats.probplot(df_num.SAT, dist = \"norm\", plot = pylab)\n","\n","\n","# In[87]:\n","\n","\n","# Checking whether data is normally distributed\n","stats.probplot(df_num.Top10, dist = \"norm\", plot = pylab)\n","\n","\n","# In[88]:\n","\n","\n","# Checking whether data is normally distributed\n","stats.probplot(df_num.Accept, dist = \"norm\", plot = pylab)\n","\n","\n","# In[89]:\n","\n","\n","# Checking whether data is normally distributed\n","stats.probplot(df_num.SFRatio, dist = \"norm\", plot = pylab)\n","\n","\n","# In[90]:\n","\n","\n","# Checking whether data is normally distributed\n","stats.probplot(df_num.Expenses, dist = \"norm\", plot = pylab)\n","\n","\n","# In[91]:\n","\n","\n","# Checking whether data is normally distributed\n","stats.probplot(df_num.GradRate, dist = \"norm\", plot = pylab)\n","\n","\n","# ### Concatenate the Entire Data\n","\n","# In[92]:\n","\n","\n","df1 = pd.concat([df_cate, df_num], axis = 1)  # Preprocessed data for clustering\n","df1.head()\n","\n","\n","# In[55]:\n","\n","\n","df1.shape\n","\n","\n","# In[94]:\n","\n","\n","df1.info()\n","\n","\n","# ## Bivariate Analysis\n","\n","# **Scatter Plot**\n","\n","# In[95]:\n","\n","\n","plt.scatter(x = df1['SAT'], y = df1['Top10'])\n","\n","\n","# In[96]:\n","\n","\n","sns.pairplot(df_num)   # Use numerical variables for scatter plots\n","\n","\n","#\n","# **Heatmap**\n","\n","# In[98]:\n","\n","\n","corrmatrix = df_num.corr(method = \"pearson\")\n","\n","sns.heatmap(corrmatrix, xticklabels = corrmatrix.columns, yticklabels = corrmatrix.columns, cmap = \"coolwarm\")\n","plt.title('Heat Map of Correlation Matrix')\n","plt.tight_layout()\n","plt.show()\n","\n","\n","# ### 9) Scaling using Normalization:\n","#\n","# The scale/magnitude of the data will be converted to min = 0 and max = 1\n","\n","# In[99]:\n","\n","\n","def norm_func(i):\n","  x = (i-i.min()) / (i.max()-i.min())\n","  return(x)\n","\n","\n","# In[100]:\n","\n","\n","df_norm = norm_func(df1.iloc[:, 1:])\n","\n","\n","# In[101]:\n","\n","\n","df_norm.describe()\n","\n","\n","# # CLUSTERING MODEL BUILDING\n","\n","# ### Hierarchical Clustering - Agglomerative Clustering\n","\n","# In[102]:\n","\n","\n","# Libraries for creating dendrogram\n","from scipy.cluster.hierarchy import linkage, dendrogram\n","from sklearn.cluster import AgglomerativeClustering\n","import matplotlib.pyplot as plt\n","\n","\n","# In[66]:\n","\n","\n","plt.figure(1, figsize = (16, 8))\n","tree_plot = dendrogram(linkage(df_norm, method  = \"ward\"))\n","\n","plt.title('Hierarchical Clustering Dendrogram')\n","plt.xlabel('Index')\n","plt.ylabel('Euclidean distances')\n","plt.show()\n","\n","\n","# In[104]:\n","\n","\n","# Applying AgglomerativeClustering choosing 3 as clusters from the above dendrogram\n","hc1 = AgglomerativeClustering(n_clusters = 3, affinity = 'euclidean', linkage = 'complete')\n","\n","y_hc1 = hc1.fit_predict(df_norm)\n","y_hc1\n","\n","\n","# In[105]:\n","\n","\n","# Applying AgglomerativeClustering choosing 3 as clusters from the above dendrogram\n","hc2 = AgglomerativeClustering(n_clusters = 3, affinity = 'euclidean', linkage = 'average')\n","\n","y_hc2 = hc2.fit_predict(df_norm)\n","y_hc2\n","\n","\n","# In[106]:\n","\n","\n","# Applying AgglomerativeClustering choosing 3 as clusters from the above dendrogram\n","hc3 = AgglomerativeClustering(n_clusters = 3, affinity = 'euclidean', linkage = 'single')\n","\n","y_hc3 = hc3.fit_predict(df_norm)\n","y_hc3\n","\n","\n","# In[107]:\n","\n","\n","hc1.labels_   # Refering to the results obtained from linkage = 'complete' method\n","\n","\n","# In[108]:\n","\n","\n","cluster_labels = pd.Series(hc1.labels_)\n","\n","\n","# In[109]:\n","\n","\n","# Add Cluster labels to orignal Dataset\n","\n","df['cluster'] = cluster_labels\n","df.head()\n","\n","\n","# # Cluster Evaluation\n","\n","# In[110]:\n","\n","\n","df.head() # The final dataframe with cluster ID's\n","\n","\n","# In[74]:\n","\n","\n","df2 = df.sort_values(by = 'cluster', ascending = True)\n","df2\n","\n","\n","# In[75]:\n","\n","\n","# Aggregate by doing mean of each cluster\n","df2.iloc[:, 2:9].groupby(df2.cluster).mean()\n","\n","\n","# In[76]:\n","\n","\n","cluster0 = df2.loc[(df2.cluster == 0), :]\n","cluster0\n","\n","\n","# In[111]:\n","\n","\n","cluster1 = df2.loc[(df2.cluster == 1), :]\n","cluster1\n","\n","\n","# In[112]:\n","\n","\n","cluster2 = df2.loc[(df2.cluster == 2), :]\n","cluster2\n","\n","\n","# In[113]:\n","\n","\n","cluster0.to_csv('University_0.csv', encoding = 'utf-8')\n","import os\n","os.getcwd()\n","\n","\n","# In[114]:\n","\n","\n","cluster1.to_csv('University_1.csv', encoding = 'utf-8')\n","import os\n","os.getcwd()\n","\n","\n","# In[115]:\n","\n","\n","cluster2.to_csv('University_2.csv', encoding = 'utf-8')\n","import os\n","os.getcwd()\n","\n","\n","# ## Another way of evaluating clusters is using outliers\n","\n","# In[82]:\n","\n","\n","for i in cluster0.columns[2:8]:\n","  sns.boxplot(x = cluster0[i])\n","  plt.show()\n","\n","\n","# In[83]:\n","\n","\n","for i in cluster1.columns[2:8]:\n","  sns.boxplot(x = cluster1[i])\n","  plt.show()\n","\n","\n","# In[84]:\n","\n","\n","for i in cluster2.columns[2:8]:\n","  sns.boxplot(x = cluster2[i])\n","  plt.show()\n","\n","\n","# In[ ]:\n","\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":149},"id":"QL1eqF9YkGYZ","executionInfo":{"status":"error","timestamp":1681556541367,"user_tz":-330,"elapsed":639,"user":{"displayName":"asit das","userId":"07383070261039464984"}},"outputId":"3a45378e-3acc-41d7-d053-87c0028044ab"},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-baa502959695>\"\u001b[0;36m, line \u001b[0;32m533\u001b[0m\n\u001b[0;31m    pip install PyQt5 pyqtwebengine --user # run this if you get any warning msg and if the plots are not visible\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]}]}